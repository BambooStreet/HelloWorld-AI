{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "import os, sys\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Flask\n",
    "from flask import Flask, request. jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "# ChatGPT\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# DB\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_mongodb.vectorstores import MongoDBAtalsVectorSearch\n",
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# ???\n",
    "from utils import gpttokens, model_tokens\n",
    "# 환경변수 불러오기\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "# Config settings\n",
    "CONFIG_NAME = \"gpt_config.json\"\n",
    "INCLUDE_RAG = True\n",
    "THRESHOLD = 0.25\n",
    "\n",
    "print(\"## config_name : \", CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'configs/{CONFIG_NAME}', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "if config['db'] == 'elasticsearch':\n",
    "    os.environ[\"ES_CLOUD_ID\"] = os.getenv(\"ES_CLOUD_ID\")\n",
    "    os.environ[\"ES_USER\"] = os.getenv(\"ES_USER\")\n",
    "    os.environ['ES_PASSWORD'] = os.getenv(\"ES_PASSWORD\")\n",
    "    os.environ[\"ES_API_KEY\"] = os.getenv(\"ES_API_KEY\")\n",
    "\n",
    "elif config['db'] == 'mongo':\n",
    "   os.environ[\"MONGODB_ATLAS_CLUSTER_URI\"] = os.getenv(\"MONGODB_ATLAS_CLUSTER_URI\")\n",
    "   api_key = os.getenv('MONGODB_API_KEY')\n",
    "   cluster_url = \"helloworld-ai.fpdjl.mongodb.net\" \n",
    "   uri = f\"mongodb+srv://{api_key}@{cluster_url}/?authMechanism=MONGODB-AWS&authSource=$external\"\n",
    "   print('## db : ',config['db'])\n",
    "   print('## db_name : ',config['path']['db_name'])\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(filename='./model/log_file.txt', level=logging.INFO, \n",
    "                    format=\"[ %(asctime)s | %(levelname)s ] %(message)s\", \n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if INCLUDE_RAG == False:\n",
    "    print(\"## No RAG system ##\")\n",
    "    logger.info(\"## No RAG system ##\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 기록을 저장할 리스트\n",
    "conversations = []; conversations_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DB once at startup\n",
    "db = None\n",
    "\n",
    "try:\n",
    "    if config['db'] == 'elasticsearch':\n",
    "        db = ElasticsearchStore(\n",
    "            index_name='helloworld',\n",
    "            embedding=OpenAIEmbeddings()\n",
    "        )\n",
    "    elif config['db'] == 'mongo':\n",
    "        client = MongoClient(os.environ[\"MONGODB_ATLAS_CLUSTER_URI\"], ssl=True,\n",
    "                             tlsCAFile='/etc/ssl/certs/ca-certificates.crt')\n",
    "        #client = MongoClient(os.environ[\"MONGODB_ATLAS_CLUSTER_URI\"],tls=True, tlsInsecure=False)\n",
    "        \n",
    "        MONGODB_COLLECTION = client[config['path']['db_name']][config['path']['collection_name']]\n",
    "        db = MongoDBAtlasVectorSearch(\n",
    "            collection = MONGODB_COLLECTION,\n",
    "            embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "            index_name = config['path']['index_name'],\n",
    "            relevance_score_fn = \"cosine\" # [cosine, euclidean, dotProduct]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Wrong db value setted in config file\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading database: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local model test\n",
    "def generate_bllossom_response(query, db):\n",
    "    global conversations\n",
    "    from transformers import AutoTokenizer\n",
    "    import torch\n",
    "    from vllm import LLM, SamplingParams\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"quantized_path\"], trust_remote_code=True)\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=config['openai_chat_inference']['model'],\n",
    "        frequency_penalty=config['openai_chat_inference']['frequency_penalty'],\n",
    "        logprobs=config['openai_chat_inference']['logprobs'],\n",
    "        top_logprobs=config['openai_chat_inference']['top_logprobs'],\n",
    "        max_tokens=config['chat_inference']['max_new_tokens'],  # 최대 토큰수\n",
    "        temperature=config['chat_inference']['temperature'],  # 창의성 (0.0 ~ 2.0)\n",
    "    )\n",
    "\n",
    "    template_text = \"\"\"\n",
    "    당신은 한국의 외국인 근로자를 위한 법률 및 비자 전문 AI 어시스턴트입니다. 다음 지침을 따라 응답해 주세요:\n",
    "\n",
    "    1. 관련 문서의 정보를 바탕으로 정확하고 최신의 법률 및 비자 정보를 제공하세요.\n",
    "    2. 복잡한 법률 용어나 절차를 쉽게 설명하여 외국인 근로자가 이해하기 쉽게 답변하세요.\n",
    "    3. 불확실한 정보에 대해서는 명확히 언급하고, 공식 기관에 문의할 것을 권장하세요.\n",
    "    4. 문화적 차이를 고려하여 정중하고 친절한 태도로 응대하세요.\n",
    "    5. 필요한 경우 관련 정부 기관이나 지원 센터의 연락처를 제공하세요.\n",
    "    6. 개인정보 보호를 위해 구체적인 개인 정보를 요구하지 마세요.\n",
    "    7. 대화 기록을 참고하여 문맥에 맞는 자연스러운 응답을 제공하세요.\n",
    "    8. 사용자의 이전 질문이나 concerns를 기억하고 연관된 정보를 제공하세요.\n",
    "\n",
    "    관련 문서: \n",
    "    {context}\n",
    "\n",
    "    이전 대화:\n",
    "    {conversation_history}\n",
    "    \"\"\"\n",
    "\n",
    "    # history 기록 넣기?\n",
    "    similar_docs = db.similarity_search_with_relevance_scores(query, k=config['config']['top_k'])\n",
    "    similar_docs = similar_docs[::-1]\n",
    "\n",
    "    # for i, doc in enumerate(similar_docs):\n",
    "    #     logger.info(f\"Top-{i+1} document : {doc.page_content}\\n\\n\")\n",
    "\n",
    "\n",
    "    # 검색된 문서의 내용을 하나의 문자열로 결합\n",
    "    context = \" \".join([doc[0].page_content for doc in similar_docs if doc[1] > THRESHOLD])\n",
    "\n",
    "    # 템플릿 설정\n",
    "    prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "    # 템플릿에 값을 채워서 프롬프트를 완성\n",
    "    filled_prompt = prompt_template.format(context = context, conversation_history= conversations, user_query=query)\n",
    "\n",
    "    # truncation - context length 넘을 경우 이전 대화 기록부터 삭제\n",
    "    if config[\"config\"]['quantized_path'] == \"ywhwang/llama-3-Korean-Bllossom-8B-awq\" and model_tokens(config[\"config\"]['quantized_path'], filled_prompt) >= config[\"config\"][\"context_length\"]:\n",
    "        logger.info(\"Max length exceeded!\")\n",
    "        conversations.pop(0)\n",
    "        filled_prompt = prompt_template.format(context = context, conversation_history= conversations, user_query=query)\n",
    "    \n",
    "    messages = [\n",
    "        {'role': 'system', 'content': f\"{filled_prompt}\"},\n",
    "        {'role': 'user', 'content': f\"사용자 질문\\n{query}\"}\n",
    "    ]\n",
    "\n",
    "    chat_messages = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Create a sampling params object.\n",
    "    sampling_params = SamplingParams(temperature=config[\"chat_inference\"][\"temperature\"], \n",
    "                                     top_p=config[\"chat_inference\"][\"top_p\"], max_tokens=config[\"chat_inference\"][\"max_tokens\"])\n",
    "    \n",
    "    llm = LLM(model=config[\"config\"][\"quantized_path\"],\n",
    "          tokenizer=config[\"config\"][\"quantized_path\"],\n",
    "          trust_remote_code = True,\n",
    "          dtype=\"float16\",\n",
    "          quantization=\"AWQ\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output = llm.generate([chat_messages],\n",
    "                                sampling_params=sampling_params)\n",
    "\n",
    "    return output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 연동\n",
    "def generate_gpt_response(query, db):\n",
    "    global conversations\n",
    "    llm = ChatOpenAI(\n",
    "        model=config['openai_chat_inference']['model'],\n",
    "        frequency_penalty=config['openai_chat_inference']['frequency_penalty'],\n",
    "        logprobs=config['openai_chat_inference']['logprobs'],\n",
    "        top_logprobs=config['openai_chat_inference']['top_logprobs'],\n",
    "        max_tokens=config['chat_inference']['max_new_tokens'],  # 최대 토큰수\n",
    "        temperature=config['chat_inference']['temperature'],  # 창의성 (0.0 ~ 2.0)\n",
    "    )\n",
    "\n",
    "    template_text = \"\"\"\n",
    "    당신은 한국의 외국인 근로자를 위한 법률 및 비자 전문 AI 어시스턴트입니다. 다음 지침을 따라 응답해 주세요:\n",
    "\n",
    "    1. 관련 문서의 정보를 바탕으로 정확하고 최신의 법률 및 비자 정보를 제공하세요.\n",
    "    2. 복잡한 법률 용어나 절차를 쉽게 설명하여 외국인 근로자가 이해하기 쉽게 답변하세요.\n",
    "    3. 불확실한 정보에 대해서는 명확히 언급하고, 공식 기관에 문의할 것을 권장하세요.\n",
    "    4. 문화적 차이를 고려하여 정중하고 친절한 태도로 응대하세요.\n",
    "    5. 필요한 경우 관련 정부 기관이나 지원 센터의 연락처를 제공하세요.\n",
    "    6. 개인정보 보호를 위해 구체적인 개인 정보를 요구하지 마세요.\n",
    "    7. 이전 대화 내용을 참고하여 문맥에 맞는 자연스러운 응답을 제공하세요.\n",
    "    8. 사용자의 이전 질문이나 concerns를 기억하고 연관된 정보를 제공하세요.\n",
    "\n",
    "    관련 문서: \n",
    "    {context}\n",
    "\n",
    "    이전 대화:\n",
    "    {conversation_history}\n",
    "\n",
    "    사용자 질문:\n",
    "    {user_query}\n",
    "    \"\"\"\n",
    "    \n",
    "    similar_docs = db.similarity_search_with_relevance_scores(query, k=config['config']['top_k'])\n",
    "    similar_docs = similar_docs[::-1]\n",
    "\n",
    "    for i, doc in enumerate(similar_docs):\n",
    "        print(f\"## Top-{i+1} document : {doc}\\n\\n\")\n",
    "\n",
    "    # 검색된 문서의 내용을 하나의 문자열로 결합\n",
    "    context = \" \".join([doc[0].page_content for doc in similar_docs if doc[1] > THRESHOLD])\n",
    "\n",
    "    # 템플릿 설정\n",
    "    prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "    # 템플릿에 값을 채워서 프롬프트를 완성\n",
    "    filled_prompt = prompt_template.format(context = context, conversation_history= conversations, user_query=query)\n",
    "    \n",
    "    # truncation - context length 넘을 경우 이전 대화 기록부터 삭제\n",
    "    if config[\"openai_chat_inference\"]['model'] == \"gpt-3.5-turbo-0125\" and gpt_tokens(model_name=config[\"openai_chat_inference\"]['model'], string=filled_prompt) >= config[\"openai_chat_inference\"][\"context_length\"]:\n",
    "        logger.info(\"## Max length exceeded!\")\n",
    "        conversations.pop(0)\n",
    "        filled_prompt = prompt_template.format(context = context, conversation_history= conversations, user_query=query)\n",
    "\n",
    "    logger.info(f\"## total prompt : {filled_prompt}\")\n",
    "    output = llm.invoke(input = filled_prompt)\n",
    "    \n",
    "    return output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no rag\n",
    "def generate_no_rag_gpt_response(query, db):\n",
    "    global conversations\n",
    "    llm = ChatOpenAI(\n",
    "        model=config['openai_chat_inference']['model'],\n",
    "        frequency_penalty=config['openai_chat_inference']['frequency_penalty'],\n",
    "        logprobs=config['openai_chat_inference']['logprobs'],\n",
    "        top_logprobs=config['openai_chat_inference']['top_logprobs'],\n",
    "        max_tokens=config['chat_inference']['max_new_tokens'],  # 최대 토큰수\n",
    "        temperature=config['chat_inference']['temperature'],  # 창의성 (0.0 ~ 2.0)\n",
    "    )\n",
    "\n",
    "    template_text = \"\"\"\n",
    "    당신은 한국의 외국인 근로자를 위한 법률 및 비자 전문 AI 어시스턴트입니다. 다음 지침을 따라 응답해 주세요:\n",
    "\n",
    "    1. 정확한 최신의 법률 및 비자 정보를 제공하세요.\n",
    "    2. 복잡한 법률 용어나 절차를 쉽게 설명하여 외국인 근로자가 이해하기 쉽게 답변하세요.\n",
    "    3. 불확실한 정보에 대해서는 명확히 언급하고, 공식 기관에 문의할 것을 권장하세요.\n",
    "    4. 문화적 차이를 고려하여 정중하고 친절한 태도로 응대하세요.\n",
    "    5. 필요한 경우 관련 정부 기관이나 지원 센터의 연락처를 제공하세요.\n",
    "    6. 개인정보 보호를 위해 구체적인 개인 정보를 요구하지 마세요.\n",
    "    7. 이전 대화 내용을 참고하여 문맥에 맞는 자연스러운 응답을 제공하세요.\n",
    "    8. 사용자의 이전 질문이나 concerns를 기억하고 연관된 정보를 제공하세요.\n",
    "\n",
    "    이전 대화:\n",
    "    {conversation_history}\n",
    "\n",
    "    사용자 질문:\n",
    "    {user_query}\n",
    "    \"\"\"\n",
    "    # 템플릿 설정\n",
    "    prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "    # 템플릿에 값을 채워서 프롬프트를 완성\n",
    "    filled_prompt = prompt_template.format(conversation_history= conversations, user_query=query)\n",
    "    \n",
    "    # truncation - context length 넘을 경우 이전 대화 기록부터 삭제\n",
    "    if config[\"openai_chat_inference\"]['model'] == \"gpt-3.5-turbo-0125\" and gpt_tokens(model_name=config[\"openai_chat_inference\"]['model'], string=filled_prompt) >= config[\"openai_chat_inference\"][\"context_length\"]:\n",
    "        logger.info(\"## Max length exceeded!\")\n",
    "        conversations.pop(0)\n",
    "        filled_prompt = prompt_template.format(conversation_history= conversations, user_query=query)\n",
    "\n",
    "    logger.info(f\"## total prompt : {filled_prompt}\")\n",
    "    output = llm.invoke(input = filled_prompt)\n",
    "    \n",
    "    return output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "\n",
    "@app.route('/get_test/<param>', methods=['GET'])\n",
    "def get_echo_call(param):\n",
    "    return jsonify({\"param\": param})\n",
    "\n",
    "\n",
    "@app.route('/question', methods=['POST'])\n",
    "def question():\n",
    "    global conversations, conversations_length\n",
    "    if db is None:\n",
    "        return jsonify({\"error\": \"Database not initialized\"}), 500\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        # logger.info(f\"Received data: {data}\")  # 받은 데이터 로깅\n",
    "\n",
    "        # 새로운 쿼리 형식 처리\n",
    "        conversation = data.get('Conversation', [])\n",
    "        if not conversation:\n",
    "            return jsonify({\"error\": \"No conversation data provided\"}), 400\n",
    "\n",
    "        print('## history ##')\n",
    "        print(conversations)\n",
    "\n",
    "        # 마지막 human 발화 추출\n",
    "        user_query = next((item['utterance'] for item in reversed(conversation) if item['speaker'] == 'human'), None)\n",
    "\n",
    "        logger.info(f\"## 사용자 쿼리: {user_query}\")\n",
    "\n",
    "        if user_query == \"감사합니다\" or user_query == \"종료\" or user_query == \"quit\":\n",
    "            # log를 파일에 출력\n",
    "            file_handler = logging.FileHandler('./model/log_file.txt')\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "            print(\"## log completed!\")\n",
    "            return jsonify({\"end\": \"dialog end\"}), 200  # 200은 HTTP 성공 상태 코드입니다\n",
    "\n",
    "        # AI 응답 생성\n",
    "        if CONFIG_NAME == \"gpt_config.json\":\n",
    "            if user_query is None:\n",
    "                return jsonify({\"error\": \"No user utterance found\"}), 400\n",
    "\n",
    "            # logger.info(f\"Extracted user query: {user_query}\")  # 추출된 사용자 쿼리 로깅\n",
    "\n",
    "            if INCLUDE_RAG == True:\n",
    "                answer = generate_gpt_response(user_query, db)\n",
    "            else:\n",
    "                answer = generate_no_rag_gpt_response(user_query, db)\n",
    "\n",
    "\n",
    "            # 대화 turn 저장\n",
    "            conversations.append({\"human\": user_query})\n",
    "            conversations.append({\"system\" : answer})\n",
    "            conversations_length += gpt_tokens(model_name=config[\"openai_chat_inference\"]['model'], string=answer)\n",
    "        \n",
    "        elif CONFIG_NAME == \"bllossom_config.json\":\n",
    "            if user_query is None:\n",
    "                return jsonify({\"error\": \"No user utterance found\"}), 400\n",
    "\n",
    "            # logger.info(f\"Extracted user query: {user_query}\")  # 추출된 사용자 쿼리 로깅\n",
    "            answer = generate_bllossom_response(user_query, db)\n",
    "            # 대화 turn 저장\n",
    "            conversations.append({\"human\": user_query})\n",
    "            conversations.append({\"system\" : answer})\n",
    "\n",
    "            conversations_length += model_tokens(config[\"config\"]['quantized_path'], answer)\n",
    "\n",
    "        # logger.info(f\"Generated AI response: {answer}\")  # 생성된 AI 응답 로깅\n",
    "\n",
    "        # AI 응답을 텍스트로 직접 반환\n",
    "        return answer, 200  # 200은 HTTP 성공 상태 코드입니다\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing question: {str(e)}\")\n",
    "        return jsonify({\"error\": f\"An error occurred: {str(e)}\"}), 500\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
